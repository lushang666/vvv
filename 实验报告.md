# 个人作业实验报告

## 实验内容综述

本次实验要求我们从mindspore官网的模型中选取audio中的模型运行，但是并没有想象中那么简单，难点有以下几点：

1.  首先我们需要下载数据集，但是数据集并不是下载好了就可以，需要上传到OBS桶中，之后还需要从OBS桶中传输到notebook中，数据集动则5G，怎么上传需要思考

2.  其次，数据集准备好之后需要预处理，.wav文件很难运行一般情况下需要.npy文件，这个预处理过程也很费事，有些模型甚至没有给这方面代码需要自己去github上下载

3.  虽说官网上给出了代码和如何运行代码，但是教程写的过于不详细，根本无法按照他给的来运行，给出的代码中有很多库和包与现在的版本不兼容，需要自己查询外网不断修改。并且给出的运行教程并不能适用于任何情况，需要自己探索，慢慢修改bug

4.  requirements.txt中所需的依赖包有很多根本不能直接用pip install指令直接运行，需要自己上网找，这点很麻烦，就比如wavenet中需要pip install audio，无论如何都会报错

5.  最后就是训练时间长，华为云GPU版本的镜像最便宜也要一小时15元，真的太贵了，整个实验花了70元，我还属于做的比较快的，训练epoch设置比较小的，希望老师可以谅解，华为云真的挺贵的55555

本次实验我共完成五个模型的训练尝试：tacotron2、melgan、jasper、wavenet、dscnn

但是由于一些模型对于运行环境和代码完整度，我放弃了一些，这部分我只是尝试。完整做完并录制视频的模型是dscnn，后续会完整讲述实验过程。

下面所叙述的实验步骤和所用镜像都是基于dscnn的其他模型的实验过程和遇到的问题会在最后统一叙述。

## 实验环境及设备简述

为了完成这部分实验我煞费苦心，各种方法都尝试过，使用mindsudio、vscode尝试本地运行，使用modelarts的notebook虚拟机运行，使用modelarts的训练实例创建运行，甚至用过VMware虚拟机配置环境。最后得出一个结论，用华为云是最好的，它虽说贵，但贵有贵的道理，它给出的镜像中包含大多数的包，并且和它给出的代码文件兼容，并且运行内存和GPU显存都比较大。

我使用的modelarts镜像为：

`mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04`

`GPU: 1*T4(16GB)|CPU: 8核 32GB`

`20 GB (云硬盘EVS)`&#x20;

值得注意的是在挑选镜像时需要看自己的模型是否支持GPU有些模型只支持海思，海思虚拟机更贵，一小时20元。至于哪些模型支持GPU需要自己看readme。

我主要使用notebook中的虚拟机命令行完成训练和评估，也就是使用基于ubantu系统。

## 实验过程叙述

这部分是基于dscnn模型的，详细叙述了如何从官网下载代码并运行起来的，我会叙述中途会出现哪些bug和这些bug如何处理，远比官方readme详细。

### 实验前的准备

首先需要下载代码到本地，下面给出下载网址

`https://gitee.com/mindspore/models/tree/r1.9/research/audio/dscnn`

[models: Models of MindSpore - Gitee.com](https://gitee.com/mindspore/models/tree/r1.9/research/audio/dscnn)

进入网站之后点击下载zip包或者在本地终端输入如下指令克隆，但是你的电脑必须提前安装Git

`git clone https://gitee.com/mindspore/models.git`

下载好的代码包结构如下：

    ├── dscnn
        ├── README.md                         // descriptions about ds-cnn
        ├── scripts
        │   ├──run_download_process_data.sh   // shell script for download dataset and prepare feature and label
        │   ├──run_train_ascend.sh            // shell script for train on ascend
        │   ├──run_eval_ascend.sh             // shell script for evaluation on ascend
        │   ├──run_train_gpu.sh               // shell script for train on gpu
        │   ├──run_eval_gpu.sh                // shell script for evaluation on gpu
        ├── src
        ├   |─ model_utils
        │       ├──config.py                  // Parameter config
        │       ├──moxing_adapter.py          // modelarts device configuration
        │       ├──device_adapter.py          // Device Config
        │       ├──local_adapter.py           // local device config
        │   ├──callback.py                    // callbacks
        │   ├──dataset.py                     // creating dataset
        │   ├──download_process_data.py       // download and prepare train, val, test data
        │   ├──ds_cnn.py                      // dscnn architecture
        │   ├──log.py                         // logging class
        │   ├──loss.py                        // loss function
        │   ├──lr_scheduler.py                // lr_scheduler
        │   ├──models.py                      // load ckpt
        │   ├──utils.py                       // some function for prepare data
        ├── train.py                          // training script
        ├── eval.py                           // evaluation script
        ├── export.py                         // export checkpoint files into air/geir
        ├── requirements.txt                  // Third party open source package
        ├── default_config.yaml               // config file

之后我们将代码包上传OBS桶，这里由于数据比较小我们使用拖拽的方式。

![](1_md_files/9178df60-bbfa-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

如果数据比较大，比如训练集我们就需要使用obsutil，下载好obsutil后，启动obsutil，在obsutil中使用命令行上传数据

`obsutil config -i=AK -k=SK -e=endpoint  //ak和sk是秘钥，可以从官网找到，endpoint是自己OBS详细信息中的`

用如下指令上传，上面这条指令是建立联系

`./obsutil cp d:\temp obs://bucket-test -f -r`

但是本模型并不需要上传代码，我们会使用数据下载命令来下载数据。

### 创建notebook并上传数据

创建开发环境选择mindspore框架的镜像并选择GPU版本。

![](1_md_files/e116e1f0-bbfc-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

配置好选项之后就可以点击创建按钮了，设置硬盘容量时也许注意，一般情况15GB就足够了但是我在运行wavenet时容量达到了20.56GB，因此需要按需设置，不过可以随时扩容，一定要注意控制台信息，不然可能会导致严重错误，不得不重现创建notebook。

创建好notebook之后输入如下指令来将OBS桶中的代码或者数据集传输到notebook中（notebook其实是虚拟机，我们之后也使用的ubantu命令行给出指令）。

`import moxing as mox  `

`mox.file.copy_parallel('obs://zzm1120210488/data', '/home/ma-user/work')`

上述代码只是我个人的路径，具体的需要自行修改 ，格式就是如此。

至此所有准备工作都已经做完了，我们可以进行正式的实验了。此时的工作目录如图

![](1_md_files/ba03b560-bc43-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

### 训练模型的构建和运行

官方给出的代码是不能直接运行，一定要做出适应性的调整，有的报错是因为版本问题，有的因为云运行问题，有的是因为代码本身有问题，这些需要我们自己去不断探索修改。

#### 准备数据集

首先在modelarts中创建终端，然后先安装依赖环境输入以下代码：

```pip install -r requirements.txt```

安装之后，按照官方文档所给的步骤，下面是输入数据集下载指令：

    python src/download_process_data.py

但是运行后会有报错，我们依次修改

![](1_md_files/0bc2c330-bc46-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

我们奉行一个原则，什么库没有安装什么库，我们使用如下指令解决这个问题：

`pip install python_speech_features`

安装之后我们继续运行数据集下载指令，看看是否还有报错。不出意外肯定要出意外，果然又报错了

![](1_md_files/6d313ca0-bc46-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

但是这个并不是库没有安装，这是我们的主python文件要调用其他python文件，但是调用的是同级文件，因此不需要用`src.model_utils.config`

因为两个文件在同级文件，在云端系统会优先搜索同级别的文件夹，不会退回去搜索，所以搜索不到是正常，但是在本地运行就不会报错，这可能就是云端会出现的问题。我们就按照报错找到对应的文件进行修改就可以了，我们把src删去就可以了。

之后继续运行下载指令但是还会报错，但是和这个报错类似，只要修改一下就可以，还是删除引用同级文件的上一级文件夹目录就可以了。修改之后我们在运行下载文件指令，会显示如下，文件开始正常下载：

![](1_md_files/5dae0690-bc47-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

等待下载完成，程序会自动运行文件预处理程序，帮我们把元数据集中的.wav文件转化为.npy 文件，方便程序执行。

![](1_md_files/8f7ba8d0-bc47-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

等程序自行执行完毕之后我们数据集的准备工作就结束了，我们后面就需要修改运行程序的bug并修改配置文件来运行训练就可以了。

#### 训练模型的构建

根据官方文档，我们需要输入如下指令运行程序

    python train.py --amp_level 'O3' --device_target=GPU --train_feat_dir your train dataset dir

官方文档中还给出了在海思环境如何运行，我使用的是GPU版本的。但是还是：不出意外的话要出意外了，会有报错：

![](1_md_files/8368a100-bc4d-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

这个还是老生常谈的问题，包括同级目录下文档的时候，需要把绝对路径名删去。

后面还有一个报错：

![](1_md_files/e2016530-bc4d-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

这是因为mindspore框架下nn.Dropout中的p改为了keep\_prob，需要自己打开该文件自行修改。修改之后应该就可以运行了。

下面还需要修稿配置文件，来满足自己的训练需求。可以直接修改defult\_config.yaml文件来配置。配置文件中各个参数的作用如下：

*   config for dataset for Speech commands dataset version 1

        'data_url': 'http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz'
                                      # Location of speech training data archive on the web
        'data_dir': 'data'            # Where to download the dataset
        'train_feat_dir': 'feat'      # Where to save the feature and label of audios
        'background_volume': 0.1      # How loud the background noise should be, between 0 and 1.
        'background_frequency': 0.8   # How many of the training samples have background noise mixed in.
        'silence_percentage': 10.0    # How much of the training data should be silence.
        'unknown_percentage': 10.0    # How much of the training data should be unknown words
        'time_shift_ms': 100.0        # Range to randomly shift the training audio by in time
        'testing_percentage': 10      # What percentage of wavs to use as a test set
        'validation_percentage': 10   # What percentage of wavs to use as a validation set
        'wanted_words': 'yes,no,up,down,left,right,on,off,stop,go'
                                      # Words to use (others will be added to an unknown label)
        'sample_rate': 16000          # Expected sample rate of the wavs
        'clip_duration_ms': 10        # Expected duration in milliseconds of the wavs
        'window_size_ms': 40.0        # How long each spectrogram timeslice is
        'window_stride_ms': 20.0      # How long each spectrogram timeslice is
        'dct_coefficient_count': 20   # How many bins to use for the MFCC fingerprint

*   config for DS-CNN and train parameters of Speech commands dataset version 1

        'model_size_info': [6, 276, 10, 4, 2, 1, 276, 3, 3, 2, 2, 276, 3, 3, 1, 1, 276, 3, 3, 1, 1, 276, 3, 3, 1, 1, 276, 3, 3, 1, 1]
                                      # Model dimensions - different for various models
        'drop': 0.9                   # dropout
        'pretrained': ''              # model_path, local pretrained model to load
        'use_graph_mode': 1           # use graph mode or feed mode
        'val_interval': 1             # validate interval
        'per_batch_size': 100         # batch size for per gpu
        'lr_scheduler': 'multistep'   # lr-scheduler, option type: multistep, cosine_annealing
        'lr': 0.1                     # learning rate of the training
        'lr_epochs': '20,40,60,80'    # epoch of lr changing
        'lr_gamma': 0.1               # decrease lr by a factor of exponential lr_scheduler
        'eta_min': 0                  # eta_min in cosine_annealing scheduler
        'T_max': 80                   # T-max in cosine_annealing scheduler
        'max_epoch': 80               # max epoch num to train the model
        'warmup_epochs': 0            # warmup epoch
        'weight_decay': 0.001         # weight decay
        'momentum': 0.98              # weight decay
        'log_interval': 100           # logging interval
        'save_ckpt_path': 'train_outputs'  # the location where checkpoint and log will be saved
        'ckpt_interval': 100          # save ckpt_interval  
        'amp_level': 'O3'             # amp level for the mix precision training

*   config for DS-CNN and evaluation parameters of Speech commands dataset version 1

        'model_size_info': [6, 276, 10, 4, 2, 1, 276, 3, 3, 2, 2, 276, 3, 3, 1, 1, 276, 3, 3, 1, 1, 276, 3, 3, 1, 1, 276, 3, 3, 1, 1]
                                      # Model dimensions - different for various models
        'drop': 0.9                   # dropout
        'pretrained': ''              # model_path, local pretrained model to load
        'use_graph_mode': 1           # use graph mode or feed mode
        'val_interval': 1             # validate interval
        'per_batch_size': 100         # batch size for per gpu
        'lr_scheduler': 'multistep'   # lr-scheduler, option type: multistep, cosine_annealing
        'lr': 0.1                     # learning rate of the training
        'lr_epochs': '20,40,60,80'    # epoch of lr changing
        'lr_gamma': 0.1               # decrease lr by a factor of exponential lr_scheduler
        'eta_min': 0                  # eta_min in cosine_annealing scheduler
        'T_max': 80                   # T-max in cosine_annealing scheduler
        'max_epoch': 80               # max epoch num to train the model
        'warmup_epochs': 0            # warmup epoch
        'weight_decay': 0.001         # weight decay
        'momentum': 0.98              # weight decay
        'log_interval': 100           # logging interval
        'save_ckpt_path': 'train_outputs'  # the location where checkpoint and log will be saved
        'ckpt_interval': 100          # save ckpt_interval  
        'amp_level': 'O3'             # amp level for the mix precision training

#### 训练过程和模型评估

在训练过程中会自动调用评估程序，生成的评估结果会输出到命令行中，会显示模型在各个数据集上的正确率。如图：

&#x20;

![](1_md_files/d0aca6a0-bc52-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

可以看到正确率还是比较高的，并且是在不断提高的。

## DSCNN训练总结

dscnn是我完整做完的实验，我将源码和演示视频打包为压缩包上交。其他模型的尝试我会在后续文档中叙述。

### 我学会了使用华为云

华为云刚刚开始学习是非常困难的，因为我们习惯了在本地使用anaconda作为环境管理程序，并用vscode进行编程，很少使用github和云上资源，所以这方面我踩了不少坑。

并且包括华为云在内的很多云上机器学习平台都是使用linux内核，需要对linux熟练掌握。

modelarts的notebook在习惯之后还是比较好用的，可以选择自己需要的镜像，可以在OBS桶中上传自己的数据，是十分方便的。但是唯一的缺点就是贵，太贵了一小时15元，这没多长时间就上百了，希望华为云可以对高校学生免费开放，或者低价开放。

### 我对语音识别模型有了熟练掌握

我尝试了很多版本的模型，有语音识别的，有语音合成的等等一系列模型，我们可以用这些模型完成很多任务。在语音合成方面对抗神经网络的作用非常大，我们会使用对抗神经网络开判断合成是否正确，音波谱线拟合是否正确，只要通过长时间地训练，模型可以对很短的一段语音进行深度模拟，我们可以生成非常相似的声音谱线。这样我们就可以完全模拟别人的声音信息。

### 我学会了如何解决过程中遇到的问题

本次实验中我遇到了各种各样的问题，有的我成功找到了解决办法，有的到现在也没找到方法解决，虽然这些问题我可能没办法解决，但是我找了方法绕开他们，有些官方文档的教程不是十分详细，没办法成功运行，我只好更换模型。

在模型的构建过程中会发生各种各样的报错，我们怎么解决十分重要。如下是我在本次实验中用的解决办法：

*   查询github论坛

    这个方法真的很有效，很多具体的问题国外早就已经有解决，因此十分可靠。

*   查询csdn和知乎

    这个算是国内代码领域十分权威的网站了，我们使用这个网站基本上可以解决70%的问题。

*   询问同学

    同学间讨论问题可以提高效率，避免一些低级错误，这并不是照抄，这和抄袭并不一样。

*   询问华为官方

    我联系过官网客服，帮忙解决modelarts的问题，华为工程师专业知识是比较有效的。

*   在mindspore论坛发帖和查询

总而言之，这次的个人任务我学到了很多东西，是非常不错的一次课程，希望以后还可以有类似的课程。mindspore虽然有缺点，但是它有很多闪光点，比如运行十分迅速，兼容性非常好，兼容pytorch，在之后的课程中我们依旧会继续研究。

## 其他模型的训练与尝试

这部分的模型基本上都夭折了， 中间有很多很多问题没有解决，因此也就搁浅了，但是这其中的尝试过程却很有意思，并且对我之后的学习有借鉴意义，所以我想短暂地叙述一下。

### jasper

这个模型是我第一个尝试的模型，其中给出了模型运行需要下载的数据集，这个数据集整整10GB训练几乎需要好多天。

于是我就放弃了，但是在这个模型的尝试中我学会了使用mindsutdio，使用远程连接编程，云上训练。

### Tacotron2

这个是可以成功运行的，此模型需要在海思环境中运行，如果不是海思环境可能连安装包都无法下载，这个模型所用的数据集是LJspeech1.1，官方文档中给出了下载地址，需要下载之后自行上传到OBS之后再传入notebook虚拟机中。

下载好的数据之后需要做预处理，处理成.hdf5文件格式，输入如下命令执行：

    python generate_hdf5 --data_path /path/to/LJSpeech-1.1

运行之后如图：

![](1_md_files/9b3a3b20-bc59-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

在等待之后，会在当前目录下生成.hdf5文件之后就可以训练了。输入如下命令：

    bash run_standalone_train.sh [DATASET_PATH] [DEVICE_ID]

这过程中也会遇到与主模型训练过程遇到相同的问题，无非就是缺安装包和代码错误，一个一个修改即可，这个模型训练过程也是十分漫长的。

这个模型也十分不推荐不是很好操作，十分费事。

### melgan

这个模型是支持GPU的，可以比海思便宜些，并且数据集和上面的一样，可以避免重复下载，这个模型也是我尝试的第三个模型，由前几个模型的试错，可以说是轻车熟路了。

首先同样是数据预处理：

按照原文中的说法，需要自行下载MEL谱处理脚本，我下载之后并不能很好的处理，我就放弃了，后来才知道处理之后就是.npy文件，我就使用dscnn的处理脚本处理这个了。

数据处理之后运行也十分简单输入如下指令就可以了。

    python train.py > train.log 2>&1 &

需要注意的是这个模型还有另一个麻烦的地方就是需要下载预训练模型，并且修改配置文件，将文件的路径都设置清楚。

![](1_md_files/e7953d20-bc5a-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

### wavenet

这个模型很麻烦，运行指令会有很多参数，必须都要正确，我记得是至少三个路径参数，是非常庞杂的。

并且官方文档中给出的代码并不完整，需要在它的连接中将另一半代码下载之后拼接在一切，有一些重名的文件需要改名并再放在一起，比如train.py另一个同名需要修改为pre\_train.py。

官方文档中给出了三个实现方案势力，有三个文件夹，如下图，但是这部分训练会频频报错，并且假设你有一个文件夹结构没有按照规定配置就会运行出错，所以这个模型是十分烦人的，我也就运行了一个高斯版本之后就放弃了。

![](1_md_files/904f8380-bc5b-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

下面是预处理命令，但是要注意的是一定要修改配置文件。

    bash run.sh --stage 0 --stop-stage 0 --db-root /path_to_dataset/LJSpeech-1.1/wavs
    bash run.sh --stage 1 --stop-stage 1

预处理之后运行如下图：

![](1_md_files/1b224d30-bc5c-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

之后我们继续运行另一条指令

![](1_md_files/43694690-bc5c-11ee-b564-739978baddbb.jpeg?v=1\&type=image)

需要注意的是图中会出现warning，最好修改一下配置文件，否则后续可能会发生错误。

在处理完成之后文件夹结构如下：

    .
    ├── gaussian
        ├──conf
        ├──data
        ├──exp
        └──dump
           └──lj
              └──logmelspectrogram
                 ├──org
                 └──norm
                    ├──train_no_dev
                    ├──dev
                    └──eval

这样预处理就完成，我们就可以开始正式训练了。

训练指令为：

    bash ./scripts/run_standalone_train_gpu.sh [DEVICE_ID] [/path_to_egs/egs/gaussian/dump/lj/logmelspectrogram/norm/] [/path_to_egs/egs/gaussian/conf/gaussian_wavenet.json] [path_to_save_ckpt]

在官方文档中还有其他平台的训练指令可以参考一下。

## 实验总结

通过这次个人作业我对mindspore和华为云有了初体验，我们之前都习惯了pytorch深度学习框架，这次我体验了华为，我十分高兴，我国自己的深度学习框架已经如此成熟，我深感欣慰。

通过这次课程，我学到了很多知识和工作经验。

语音识别与合成在如今社会应用广泛，可以说只要有机器语音的地方就有它的身影，我能紧跟时代风向，我特别开心，无论是数学推理还是小实验都十分有趣。

小组合作经验也十分重要，这次我们使用github与全班同学进行合作，这是全新的体验，探索的过程确实十分艰难，但是我在全力以赴了。小组内的沟通协商也十分重要，我们需要深度合作。

## 致谢

最后，感谢在这门课中帮助过我的人，感谢我的小组成员，感谢助教学长，感谢老师。
